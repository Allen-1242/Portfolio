[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome !   ",
    "section": "",
    "text": "Hi, Iâ€™m Allen Sunny, a masterâ€™s student at the University of Maryland, College Park. I build human-centered AI systems and conduct research at the intersection of machine learning, public services, and digital accountability. Iâ€™m currently finishing my thesis under the guidance of Professor Ido Sivan-Sevilla.\nThis site showcases my research, projects, and links to my code repositories."
  },
  {
    "objectID": "index.html#current-work",
    "href": "index.html#current-work",
    "title": "Welcome !   ",
    "section": "ğŸ” Current Work",
    "text": "ğŸ” Current Work\n\nğŸ“„ Download My Resume\nğŸ“ Read My Blog"
  },
  {
    "objectID": "index.html#research-projects",
    "href": "index.html#research-projects",
    "title": "Welcome !   ",
    "section": "ğŸ“š Research Projects",
    "text": "ğŸ“š Research Projects\n\nHuman-Centered AI for the Public Sector\nDeveloping a framework and prototype to support the responsible deployment of AI in public institutions. This work draws on case studies of past failures, emphasizes human-centered design, and proposes a roadmap for post-deployment evaluation and accountability.\n\nğŸ“„ Thesis (in progress)\n\nğŸ’» Prototype coming soon\n\n\n\nTrust in Transparency: How Explainable AI Shapes User Perceptions\nA qualitative study based on semi-structured interviews that explores how users emotionally respond to AI-generated explanations. The findings highlight that trust is shaped not just by transparency, but also by reliability, clarity, and perceived control.\n- ğŸ“„ Paper\n\n\nExplainability and Trust in AI: A Quantitative Approach\nA survey based study that evaluates how different types of AI explanations influence user perceptions of fairness, trust, and decision legitimacy.\n- ğŸ“„ Paper\n\n\nOxford AI Monitoring System\nA lightweight, automated web scraper designed to track corporate adoption of foundation models. This tool supports transparency and governance efforts by mapping real-time deployment trends across public-facing websites.\n- ğŸ’» Code"
  },
  {
    "objectID": "index.html#open-source-tools",
    "href": "index.html#open-source-tools",
    "title": "Welcome !   ",
    "section": "ğŸ› ï¸ Open Source Tools",
    "text": "ğŸ› ï¸ Open Source Tools\n\nâš™ï¸ Applied Systems\n\nDark Pattern Analysis\nA browser extension that detects deceptive UX elements using DOM analysis and LLM-based classification.\n- ğŸ“„ Paper\n- ğŸ’» Code\n\n\nRADAR: Retrieval-Augmented Data Analysis and Representation\nA RAG-powered system that turns natural language into explainable data visualizations using LLaMA 3.1 8B.\n- ğŸ“„ Paper\n- ğŸ’» Code\n\n\n\n\nğŸ“¦ R Tools for Interpretable Machine Learning\n\nTangledFeatures\nPerforms feature selection in highly correlated spaces for use with interpretable models.\n- ğŸŒ Package Site\n- ğŸ“„ Documentation\n- ğŸ§  Paper\n\n\nStructuralDecompose\nDecomposes time series into components while remaining robust to level shifts.\n- ğŸŒ Package Site\n- ğŸ“„ Documentation\n- ğŸ§  Paper"
  },
  {
    "objectID": "index.html#about-this-site",
    "href": "index.html#about-this-site",
    "title": "Welcome !   ",
    "section": "ğŸŒ About This Site",
    "text": "ğŸŒ About This Site\nBuilt with Quarto\nHosted on GitHub Pages\nAlways evolving."
  },
  {
    "objectID": "papers/explainability/index.html",
    "href": "papers/explainability/index.html",
    "title": "Redirecting to Paperâ€¦",
    "section": "",
    "text": "Redirecting to paperâ€¦\nIf you are not redirected automatically, click here."
  }
]